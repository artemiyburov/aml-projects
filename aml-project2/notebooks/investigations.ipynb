{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'balanced_accuracy_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c3a8b5116a3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbalanced_accuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'balanced_accuracy_score'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "df_X_train = pd.read_csv('../data/raw/X_train.csv')\n",
    "df_y_train = pd.read_csv('../data/raw/y_train.csv')\n",
    "df_X_test = pd.read_csv('../data/raw/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id    y        x0        x1        x2        x3        x4        x5  \\\n",
      "0        0  1.0 -4.524480 -0.718917  0.827537 -2.336564  1.905992 -1.424985   \n",
      "1        2  1.0 -0.547026 -0.045593  1.016072 -0.068002 -0.670472 -0.551299   \n",
      "2        3  1.0 -1.939258 -0.284554  1.276007 -0.500731  1.088817 -0.897736   \n",
      "3        4  1.0 -0.386835 -0.143997  0.506509 -0.648928 -0.614121  0.211504   \n",
      "4        5  1.0 -1.933678 -0.645258  0.762559 -0.065472  0.930215 -0.576592   \n",
      "5        7  1.0 -2.566073 -0.896698  0.390199 -0.448086  1.524759 -0.049982   \n",
      "6        8  1.0 -4.037710 -0.995334  2.562033 -0.452974  1.742658 -2.414891   \n",
      "7        9  1.0 -1.447890 -0.006329  0.103616 -1.350514 -0.217469 -0.584939   \n",
      "8       11  1.0 -1.838458  0.324521  0.359177 -1.781189  3.102764 -0.202826   \n",
      "9       13  1.0 -1.621783 -1.584052  1.529401 -1.005307  0.338492 -1.382978   \n",
      "10      14  1.0 -2.999272 -0.847908  0.880497 -0.514934  1.020666 -0.905973   \n",
      "11      16  1.0 -0.020872 -0.211064 -0.252272  0.933852 -0.803811  0.923688   \n",
      "12      17  1.0  0.119826 -0.312168  0.529867  0.117930 -0.474165 -0.045623   \n",
      "13      18  1.0 -2.991126 -1.922787  1.335772 -1.667513  0.782848 -0.487829   \n",
      "14      19  1.0 -0.140653  0.279008  0.178039  0.163582 -1.035937 -0.149504   \n",
      "15      20  1.0 -3.368564 -0.419367  1.424706 -1.557114  1.842390 -0.112904   \n",
      "16      22  1.0 -0.886689  0.034795 -0.176630  0.118427 -0.918382  0.501684   \n",
      "17      26  1.0 -0.979073 -0.437862  0.070629 -0.255112 -0.814221  0.067584   \n",
      "18      27  1.0 -2.907311 -2.312611  1.199738  0.577149  0.540623 -0.367044   \n",
      "19      28  1.0 -0.682010  0.044270  0.346068  0.707455 -0.771027  0.721352   \n",
      "20      29  1.0 -2.740908 -0.850701  1.860183  0.307707  1.076327 -1.859382   \n",
      "21      30  1.0 -2.910523 -1.601861  0.773849 -2.011684  1.144315 -0.304363   \n",
      "22      31  1.0 -0.096307  0.499388  0.057492 -0.305270 -0.240226  0.656983   \n",
      "23      32  1.0 -0.321833 -0.464352  0.241643 -0.078966 -0.018931  0.979963   \n",
      "24      33  1.0 -3.531536 -1.603576  0.344529 -1.700742  1.783826 -1.160933   \n",
      "25      34  1.0 -2.462088 -0.303209  2.131123  0.142328  0.704543 -1.548879   \n",
      "26      35  1.0 -0.060954  0.382147  0.373634 -0.220295 -0.560426  0.608963   \n",
      "27      36  1.0 -0.383317  0.279294  0.559829  0.077461 -1.127133 -0.107462   \n",
      "28      37  1.0 -0.884416  0.188703 -0.527745  0.125118 -0.547897  0.762546   \n",
      "29      38  1.0  0.125654 -0.179001 -0.062763 -0.409077 -0.258189  0.169732   \n",
      "...    ...  ...       ...       ...       ...       ...       ...       ...   \n",
      "1770  4674  0.0 -1.585758  0.163884 -1.279225 -0.692817 -0.129755  0.246913   \n",
      "1771  4681  2.0 -0.438263  0.566014 -0.118967  0.102344  0.133764  1.167104   \n",
      "1772  4682  2.0  0.048926 -0.411590 -0.198732  0.648774 -0.772037  0.913483   \n",
      "1773  4683  2.0  0.148009 -0.436011 -0.401828  0.451298 -0.792145  1.122783   \n",
      "1774  4684  0.0 -0.437835  0.534868  0.498771  0.549582  0.614124  0.244860   \n",
      "1775  4690  0.0  0.007723 -0.053962  0.522970  0.207078 -0.453187  0.393444   \n",
      "1776  4696  2.0 -0.159054  0.027398 -0.258594  0.838429 -0.883970  0.577626   \n",
      "1777  4697  2.0 -2.206393 -0.489322  1.474813  0.010000  0.735636  0.399038   \n",
      "1778  4699  0.0 -0.261664 -0.515654  0.995974  0.960053 -0.968401  0.750870   \n",
      "1779  4702  0.0  0.368319 -0.036882  0.078434  0.864154 -0.989759  0.745960   \n",
      "1780  4713  2.0 -0.120440 -0.051649  0.104111  0.387602 -0.330102  0.479069   \n",
      "1781  4715  2.0 -0.364321 -0.202281 -0.156739  0.366465 -0.293405  0.753584   \n",
      "1782  4716  0.0 -1.162151 -0.801002  0.191822  0.426245 -0.215364 -0.659251   \n",
      "1783  4728  2.0 -0.304611 -0.764593 -0.558323  1.031724 -0.792901  1.318132   \n",
      "1784  4730  2.0 -0.033328  0.192783 -0.109219  0.829090 -0.625990  0.907893   \n",
      "1785  4743  2.0 -0.934918 -0.300898  0.481093  0.431348 -0.793314  0.620429   \n",
      "1786  4744  2.0 -1.386504  0.578913  0.105997  0.305070 -1.387136  0.898515   \n",
      "1787  4747  2.0  0.425354 -0.386897  0.075913  0.743093 -0.499274  0.269098   \n",
      "1788  4748  0.0 -0.681440 -0.686171 -0.449663  0.720160 -0.653079  0.109212   \n",
      "1789  4768  2.0 -0.137150 -0.059354  0.119480  0.596607 -0.398642  0.693623   \n",
      "1790  4773  0.0 -0.305430 -0.216893  0.202446  0.188797 -1.228087  1.411828   \n",
      "1791  4777  2.0 -1.551535 -1.082366  1.198819  0.839871  0.075184  1.957636   \n",
      "1792  4781  0.0 -0.922095  0.035247  0.262330 -0.037101 -1.621889  0.653470   \n",
      "1793  4782  2.0 -0.528788 -0.328577  0.087995  0.406471 -0.495917  0.451469   \n",
      "1794  4783  0.0 -0.181992  0.564874  0.024390 -0.161031  0.139696  0.189843   \n",
      "1795  4790  2.0  0.253880 -0.385975  0.854385  0.168807 -1.418435  1.251242   \n",
      "1796  4791  0.0 -1.945461 -0.557256  1.000248 -1.194476 -0.136970 -1.916035   \n",
      "1797  4794  0.0 -0.427221 -0.013163 -0.207569  0.185657 -1.028522  0.757112   \n",
      "1798  4798  0.0 -0.314944 -0.135876 -0.016729  0.370428 -0.498063  0.326551   \n",
      "1799  4799  0.0 -0.897005  0.480667  0.427190  0.656980 -0.319640  0.382714   \n",
      "\n",
      "            x6        x7    ...         x990      x991      x992      x993  \\\n",
      "0    -5.622933 -0.739429    ...    -3.214350 -4.083583 -1.240234  1.581522   \n",
      "1    -0.550926  0.393147    ...     0.064608 -0.361322 -0.440028  0.278972   \n",
      "2    -1.530660 -0.952914    ...    -2.341105 -1.663250 -1.224091  0.617387   \n",
      "3     0.025600 -0.272372    ...     0.083148  0.568495  0.050980  0.317415   \n",
      "4    -2.845181 -1.057360    ...    -1.475300 -1.809072  0.400202 -0.412231   \n",
      "5    -1.456666 -0.125867    ...    -1.056658 -1.479102 -0.512893  1.308599   \n",
      "6    -2.589802  0.094407    ...    -3.214731 -2.948664 -1.235238  2.183062   \n",
      "7    -1.478773  0.102207    ...    -1.179465 -1.038654 -0.076826  0.325928   \n",
      "8    -2.585042 -2.070370    ...    -1.618504 -2.148769 -1.263612  0.206546   \n",
      "9    -1.745269 -0.705736    ...    -1.083272 -2.320164 -1.080334  0.709370   \n",
      "10   -2.780507 -1.206887    ...    -1.800565 -2.625756 -0.942068  1.262711   \n",
      "11   -0.166921  0.146144    ...     0.538375 -0.067323  0.667729 -0.218013   \n",
      "12   -0.384345 -0.363680    ...    -0.221794 -0.106031 -0.034972  0.349913   \n",
      "13   -3.076019 -1.193804    ...    -2.242840 -2.326117 -0.454402  1.239631   \n",
      "14   -0.273933 -0.300271    ...     0.197508  0.710859  0.379661  0.749090   \n",
      "15   -1.822198 -1.025768    ...    -0.818670 -1.112991 -0.790199  2.445245   \n",
      "16    0.221979  0.208183    ...     0.794280  0.424576  0.295981  0.218170   \n",
      "17   -0.018351 -0.280316    ...     0.155194 -0.252836  0.241845 -0.044057   \n",
      "18   -2.686335 -1.619022    ...    -1.774924 -2.176857 -0.361875  1.281941   \n",
      "19    0.448925 -0.145711    ...     0.158600  0.271774  0.505902  0.318355   \n",
      "20   -4.150657 -0.989259    ...    -1.890975 -3.258724 -1.223616  1.138324   \n",
      "21   -2.396021 -1.413479    ...    -2.071976 -1.836900 -2.339803  1.118345   \n",
      "22   -0.750998  0.328405    ...    -0.268984 -0.067956 -0.024773  0.796698   \n",
      "23   -0.566055 -0.585079    ...     0.230836 -1.060634 -0.424336 -0.006786   \n",
      "24   -1.472051 -0.128920    ...    -1.618545 -2.785622 -0.681933  2.590274   \n",
      "25   -2.708458 -1.591024    ...    -1.833540 -1.589555 -0.637304  0.341239   \n",
      "26    0.498259  0.260124    ...     0.347659 -0.518900  0.094924  0.537129   \n",
      "27   -0.963221  0.338454    ...     0.474391  0.124699 -0.084501  0.458747   \n",
      "28   -0.111799 -0.212514    ...     0.681872 -0.507303 -0.650071  0.202889   \n",
      "29   -0.682944 -0.413361    ...    -0.153110  0.203450 -0.274487 -0.139414   \n",
      "...        ...       ...    ...          ...       ...       ...       ...   \n",
      "1770 -0.718615  0.300872    ...     0.127607 -0.724535 -0.574143  0.594416   \n",
      "1771 -0.612772 -0.885911    ...    -0.489617  0.155901 -0.086239 -0.459747   \n",
      "1772 -0.385324 -0.087360    ...     0.437330  0.448600  0.292914  0.110404   \n",
      "1773  0.111860  0.454960    ...     0.327613  0.163405 -0.239783  0.268291   \n",
      "1774 -0.958769 -0.870202    ...    -0.435251  0.109471 -0.067516  0.256823   \n",
      "1775  0.118187 -0.009181    ...     0.483541  0.429286  0.728763  0.247534   \n",
      "1776 -0.195346  0.075640    ...     0.177081  0.192339  0.212524  0.470497   \n",
      "1777 -0.610472  0.012531    ...    -1.222896 -0.608547 -0.885318  0.394425   \n",
      "1778 -0.867810 -0.484383    ...    -0.936442 -0.599027  0.261143  0.018903   \n",
      "1779  0.092914 -0.159223    ...     0.802586  0.781849  0.310435  0.290113   \n",
      "1780 -0.091533  0.225362    ...     0.076826  0.038085  0.189598  0.327536   \n",
      "1781  0.326808  0.046413    ...     0.469647  0.552091  0.402930  0.745971   \n",
      "1782 -0.019038 -1.458269    ...    -0.214239  0.116113  0.506449  0.705049   \n",
      "1783  0.263783  0.556192    ...    -0.312419  0.097447  1.622339 -0.146339   \n",
      "1784  0.394209 -0.196342    ...     0.710879  0.295297  0.665164  0.154541   \n",
      "1785  0.591097  0.323404    ...     0.376552 -0.204610  0.022809  0.543526   \n",
      "1786 -1.800990  0.059011    ...     1.209044 -0.543116  0.141141  1.860235   \n",
      "1787  0.423992  0.510407    ...     0.290523  0.502643 -0.584575  0.433401   \n",
      "1788 -0.816327  0.693221    ...     1.305696  0.120154  1.783191  0.203421   \n",
      "1789  0.265014  0.136982    ...     0.485991  0.003776  0.471576  0.945758   \n",
      "1790 -0.152991  0.161886    ...     0.506373  0.277645  0.441768  0.227781   \n",
      "1791 -0.649735 -0.502340    ...    -0.937265 -0.072726 -0.023557 -0.529673   \n",
      "1792 -0.659658 -0.117562    ...     0.126779 -0.131369  0.415523  0.032410   \n",
      "1793 -0.137798 -0.041730    ...     0.677212 -0.257708 -0.229908  0.695604   \n",
      "1794 -0.809355  0.596488    ...     0.046470  0.350837  0.642119  0.709318   \n",
      "1795 -0.276148  0.401376    ...     0.494057 -0.000770  0.829315 -0.211487   \n",
      "1796 -2.439545 -0.857608    ...    -0.754797 -0.815857  0.220747  1.467021   \n",
      "1797  0.256525  0.353054    ...    -0.045911  0.194378  0.023417  0.337711   \n",
      "1798  0.171792  0.365708    ...     0.114970  0.419574  0.623070  0.401182   \n",
      "1799 -0.007508 -0.547511    ...     0.081136  0.814874  0.797315 -0.715734   \n",
      "\n",
      "          x994      x995      x996      x997      x998      x999  \n",
      "0    -3.147444  0.423618  2.387999  1.784247 -1.689361 -1.586569  \n",
      "1    -0.570960 -0.708099 -0.025025  0.552631 -1.365591 -0.584266  \n",
      "2    -0.964099 -0.034949  0.157197  0.137123 -0.165070 -0.740363  \n",
      "3    -0.163551 -0.240090 -0.270020 -0.296239 -0.722527  0.986404  \n",
      "4    -2.140778 -0.592177 -0.207706  1.337099 -0.067694 -1.120466  \n",
      "5    -1.199814  0.240815  0.284197  0.868174 -0.673176 -0.414345  \n",
      "6    -2.329619  0.966741  0.888183  1.378246 -1.165609 -0.759809  \n",
      "7    -0.301200  0.271853 -0.175449  0.338832 -0.636837 -0.463838  \n",
      "8    -2.174126  0.631257 -0.052601  0.063660 -1.428014 -0.630883  \n",
      "9    -2.570957  0.593407  0.306215  0.794985 -0.569983 -1.917891  \n",
      "10   -1.360106  0.669568  0.306596  0.954491 -1.037827 -0.936841  \n",
      "11    0.139194  0.305060  0.436842  0.100339 -0.408948  0.371572  \n",
      "12   -0.837260  0.665798  0.658889  0.440352 -0.561855  0.015917  \n",
      "13   -2.204412  0.834956  0.624796  0.449245 -2.272878 -3.091086  \n",
      "14   -0.183757 -0.437195 -0.216130  0.429175 -0.584734 -0.021733  \n",
      "15   -1.307024  0.377685  1.731882  1.916635 -1.441884 -0.427556  \n",
      "16   -1.015816 -0.021908  0.411806 -0.568237  0.143582  0.464098  \n",
      "17   -0.370235  0.050732  0.266510  0.117426 -0.083884  0.619883  \n",
      "18   -1.993267  0.282936  0.834844  1.088345  0.396427  1.207057  \n",
      "19   -0.258199 -0.291541  0.234796 -0.023774 -0.298483  0.254589  \n",
      "20   -2.253941  1.962802 -0.266191  0.331974 -0.632990 -0.639609  \n",
      "21   -1.666837  1.210748  1.396321  0.714935 -1.246902  0.583912  \n",
      "22   -0.729791 -0.081198 -0.020374  0.471498 -0.406545  0.351299  \n",
      "23    0.111404  0.649617  1.161924  1.042213 -0.775298  0.046705  \n",
      "24   -0.556821  0.514694  0.524665  0.573712  0.493559 -1.117167  \n",
      "25   -2.304004 -0.066649  0.432849  0.896042 -1.198455 -1.549725  \n",
      "26   -0.460218  0.395940 -0.276497  0.767647 -0.410674  0.432631  \n",
      "27   -1.043398 -0.124382  0.042562 -0.191489 -0.310205  0.624262  \n",
      "28    0.032566 -0.221624 -0.556763  0.128119 -0.806555  0.230710  \n",
      "29   -0.066493  0.325079  0.330866  0.338747 -0.669512  0.302607  \n",
      "...        ...       ...       ...       ...       ...       ...  \n",
      "1770 -0.307496 -0.433936 -0.639234  0.302735 -0.545653 -0.072369  \n",
      "1771 -0.162573 -0.244912 -0.351983 -0.248787 -1.351929  0.557544  \n",
      "1772  0.284140 -0.096940 -0.155365 -0.157111 -0.234847  1.068993  \n",
      "1773 -0.029520 -0.241499 -0.071496 -0.270612 -0.692890  0.750275  \n",
      "1774 -0.367368 -0.391200  0.118656  0.937934  0.195970 -0.243643  \n",
      "1775 -0.262014 -0.564019 -0.247082 -0.092777  0.199395  0.011424  \n",
      "1776  0.219355 -0.468801  0.064590 -0.130588 -0.577425  0.712009  \n",
      "1777 -0.898959  1.199502  0.570601  0.873922 -1.100964 -0.089528  \n",
      "1778  0.245730  0.450130  1.146285  0.724200 -0.940359 -0.233244  \n",
      "1779  0.497264 -0.025675  0.081855  0.263110  0.178853  0.691370  \n",
      "1780 -0.018516 -0.230259  0.273856 -0.122568 -0.191285  0.467489  \n",
      "1781 -0.002560 -0.026985  0.612596  0.078952 -0.359981  0.675060  \n",
      "1782 -1.197850 -0.654351 -0.580494  1.448633 -0.316671 -0.095800  \n",
      "1783  0.847985 -0.055458  0.242740  0.312715  0.045949  0.782790  \n",
      "1784  0.065484 -0.079636  0.133200 -0.153315 -0.374214  0.878949  \n",
      "1785 -0.372408 -0.179947 -0.443895 -0.091931  0.135887  0.554163  \n",
      "1786 -1.026902 -0.339121  0.088924 -0.560337 -0.190993  1.710143  \n",
      "1787  0.476068  0.059520  0.041415  0.179930  0.116840  0.970794  \n",
      "1788 -1.464064 -0.723545 -0.513142 -0.252106  0.426544  0.152259  \n",
      "1789  0.155240  0.265518  0.080853  0.578584 -0.227646  0.529959  \n",
      "1790  0.265185 -0.254526  0.481724 -0.304787  0.372740  0.312763  \n",
      "1791 -0.212504 -0.811027 -1.063504  0.270992 -0.875453  1.019706  \n",
      "1792 -0.684503 -0.355968 -0.339716  0.002230 -0.421359  0.184183  \n",
      "1793 -0.089965 -0.008558  0.451951  0.668318  0.318121  0.400262  \n",
      "1794 -0.638614  0.011524 -0.216699  0.954319  0.042014  0.046263  \n",
      "1795  0.230609 -0.383896  0.121569 -0.441183  0.142386  0.984760  \n",
      "1796 -1.614342 -0.301911 -1.203142  0.547425 -0.210862 -0.692484  \n",
      "1797  0.253177 -0.271093 -0.030959  0.090284 -0.006211  0.617743  \n",
      "1798 -0.321963 -0.009893 -0.620517  0.007603  0.044003  0.164154  \n",
      "1799 -0.169048 -0.198681 -0.348088  0.147589  0.011381  0.168418  \n",
      "\n",
      "[1800 rows x 1002 columns]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.merge( df_y_train, df_X_train, on='id')\n",
    "df_train = pd.concat([df_train.loc[df_train['y'] == 1].iloc[:600], df_train.loc[df_train['y'] != 1]]).reset_index().drop('index', axis = 1)\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train\n",
    "Y = X['y'].values\n",
    "X = X.drop('y', axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "pipelines.append(('ScaledADA', Pipeline([('Scaler', RobustScaler()),('ADA', AdaBoostClassifier())])))\n",
    "pipelines.append(('ScaledGBM', Pipeline([('Scaler', RobustScaler()),('GBM', GradientBoostingClassifier())])))\n",
    "pipelines.append(('ScaledGPR', Pipeline([('Scaler', RobustScaler()),('GPR', RandomForestClassifier())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1800, 1440]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ac8e6739cc83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \"\"\"\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1800, 1440]"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "scorer = make_scorer()\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10, random_state=42)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scorer)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = Y\n",
    "X_test = df_X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler().fit(X_train)\n",
    "rescaled_X_train = scaler.transform(X_train)\n",
    "rescaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "model.fit(rescaled_X_train, y_train)\n",
    "y_pred = model.predict(rescaled_X_test) #predicted values without index column\n",
    "df_y_pred = pd.DataFrame({'id': np.array(df_X_test['id']),'y': y_pred})\n",
    "df_y_pred.to_csv('../data/processed/y_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(n_classifiers=np.arange(1,200))\n",
    "model = Pipeline([('GBM', GradientBoostingRegressor())])\n",
    "kfold = KFold(n_splits=10, random_state=42)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='r2', cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
